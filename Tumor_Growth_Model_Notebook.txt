{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tumor Growth Modeling with Numerical Solver and PINN\n",
        "\n",
        "## Overview\n",
        "This notebook implements a tumor growth model using a numerical PDE solver to generate ground truth data and a Physics-Informed Neural Network (PINN) to approximate the solution. The model simulates the spatial and temporal dynamics of normal cells (Nn), tumor cells (Nt), and excess hydrogen ions (Ch) in a radial domain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from scipy.integrate import solve_ivp\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Numerical Solver\n",
        "\n",
        "### Model Parameters\n",
        "The `ModelParameters` class defines the grid, physical parameters, and normalization scales for the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class ModelParameters:\n",
        "    def __init__(self):\n",
        "        # Grid and model parameters\n",
        "        self.rl, self.ru, self.n = 0.0, 0.5, 101\n",
        "        self.r = np.linspace(self.rl, self.ru, self.n)\n",
        "        self.dr = self.r[1] - self.r[0]\n",
        "        self.rn1, self.rn2, self.Kn = 1.0e-06, 1.0, 5.0e+07\n",
        "        self.rt1, self.Dt, self.Kt = 1.0e-06, 2.0e-10, 5.0e+07\n",
        "        self.rh1, self.rh2, self.Dh = 2.2e-17, 1.1e-04, 5.0e-06\n",
        "        self.ncall = 0\n\n",
        "        # Characteristic scales for normalization\n",
        "        self.Nn_scale = 1.0e+08\n",
        "        self.Nt_scale = 1.0e+07\n",
        "        self.Ch_scale = 1.0e-07\n",
        "        self.t_scale = 5.0e+06\n",
        "        self.r_scale = 0.5\n\n",
        "        # Add minimum radius to avoid division by zero\n",
        "        self.r_min = 1e-8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PDE System\n",
        "The `numerical_pde_system` function defines the system of partial differential equations governing the dynamics of Nn, Nt, and Ch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def numerical_pde_system(t, u, params):\n",
        "    params.ncall += 1\n",
        "    n = params.n\n",
        "    Nn, Nt, Ch = u[0:n], u[n:2*n], u[2*n:3*n]\n\n",
        "    Nnr, Ntr, Chr = np.gradient(Nn, params.dr), np.gradient(Nt, params.dr), np.gradient(Ch, params.dr)\n",
        "    Ntr[0], Ntr[-1] = 0, 0  # Boundary conditions\n",
        "    Chr[0], Chr[-1] = 0, 0  # Boundary conditions\n\n",
        "    Ntrr, Chrr = np.gradient(Ntr, params.dr), np.gradient(Chr, params.dr)\n\n",
        "    Nnt, Ntt, Cht = np.zeros(n), np.zeros(n), np.zeros(n)\n\n",
        "    for i in range(n):\n",
        "        D = params.Dt * (1 - Nn[i] / params.Kn)\n",
        "        if D < 0: D = 0\n\n",
        "        Nnt[i] = params.rn1 * Nn[i] * (1 - Nn[i] / params.Kn) - params.rn2 * Ch[i] * Nn[i]\n",
        "        if i == 0:\n",
        "            Ntt[i] = params.rt1 * Nt[i] * (1 - Nt[i] / params.Kt) + 3 * D * Ntrr[i]\n",
        "            Cht[i] = params.rh1 * Nt[i] - params.rh2 * Ch[i] + 3 * params.Dh * Chrr[i]\n",
        "        else:\n",
        "            diffusion_Nt = D * (Ntrr[i] + 2/params.r[i] * Ntr[i]) + (-params.Dt/params.Kn) * Nnr[i] * Ntr[i]\n",
        "            Ntt[i] = params.rt1 * Nt[i] * (1 - Nt[i] / params.Kt) + diffusion_Nt\n",
        "            diffusion_Ch = params.Dh * (Chrr[i] + 2/params.r[i] * Chr[i])\n",
        "            Cht[i] = params.rh1 * Nt[i] - params.rh2 * Ch[i] + diffusion_Ch\n\n",
        "    return np.concatenate((Nnt, Ntt, Cht))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Initial Conditions\n",
        "The `get_initial_conditions` function sets up the initial distributions for Nn, Nt, and Ch using hyperbolic tangent functions to create smooth transitions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_initial_conditions(params):\n",
        "    r, n = params.r, params.n\n",
        "    u0 = np.zeros(3 * n)\n",
        "    rs = 50\n",
        "    r_transition_cells, r_transition_H = r[20], r[10]\n\n",
        "    tanhr_Nn = np.tanh(rs * (r - r_transition_cells))\n",
        "    u0[0:n] = 5.0e+07 * (1 - tanhr_Nn) / 2 + 1.0e+08 * (1 + tanhr_Nn) / 2\n\n",
        "    tanhr_Nt = np.tanh(rs * (r - r_transition_cells))\n",
        "    u0[n:2*n] = 1.0e+05 * (1 - tanhr_Nt) / 2 + 1.0e+03 * (1 + tanhr_Nt) / 2\n\n",
        "    tanhr_Ch = np.tanh(rs * (r - r_transition_H))\n",
        "    u0[2*n:3*n] = 1.0e-09 * (1 - tanhr_Ch) / 2\n",
        "    return u0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ground Truth Data Generation\n",
        "The `generate_ground_truth_data` function runs the numerical solver, adds 1% noise to the solution, and saves the data to a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generate_ground_truth_data(params):\n",
        "    print(\"--- Running Numerical Solver to Generate Ground Truth Data ---\")\n",
        "    t0, tf, nout = 0.0, 5.0e+06, 21\n",
        "    tout = np.linspace(t0, tf, nout)\n",
        "    u0 = get_initial_conditions(params)\n\n",
        "    sol = solve_ivp(numerical_pde_system, [t0, tf], u0, method='BDF', t_eval=tout, args=(params,))\n\n",
        "    if not sol.success:\n",
        "        raise RuntimeError(\"Numerical ODE solver failed:\", sol.message)\n\n",
        "    u = sol.y.T\n",
        "    Nn, Nt, Ch = u[:, :params.n], u[:, params.n:2*params.n], u[:, 2*params.n:3*params.n]\n\n",
        "    noise_level = 0.01 # 1% noise\n",
        "    Nn_noisy = Nn + noise_level * np.std(Nn) * np.random.randn(*Nn.shape)\n",
        "    Nt_noisy = Nt + noise_level * np.std(Nt) * np.random.randn(*Nt.shape)\n",
        "    Ch_noisy = Ch + noise_level * np.std(Ch) * np.random.randn(*Ch.shape)\n\n",
        "    csv_filename = 'tumor_growth_data.csv'\n",
        "    with open(csv_filename, 'w', newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(['t', 'r', 'Nn', 'Nt', 'Ch'])\n",
        "        for it, t_val in enumerate(tout):\n",
        "            for i, r_val in enumerate(params.r):\n",
        "                writer.writerow([t_val, r_val, Nn_noisy[it, i], Nt_noisy[it, i], Ch_noisy[it, i]])\n",
        "    print(f\"Noisy ground truth data saved to {csv_filename}\\n\")\n",
        "    return tout, params.r, Nn, Nt, Ch, Nn_noisy, Nt_noisy, Ch_noisy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Physics-Informed Neural Network (PINN)\n",
        "\n",
        "### PINN Architecture\n",
        "The `PINN` class defines a neural network with dense layers and tanh activations, designed to output normalized solutions for Nn, Nt, and Ch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class PINN(tf.keras.Model):\n",
        "    def __init__(self, layers, params):\n",
        "        super(PINN, self).__init__()\n",
        "        self.params = params\n",
        "        self.hidden = []\n",
        "        for units in layers[1:-1]:\n",
        "            layer = tf.keras.layers.Dense(\n",
        "                units,\n",
        "                activation='tanh',\n",
        "                kernel_initializer='glorot_normal',\n",
        "                bias_initializer='zeros'\n",
        "            )\n",
        "            self.hidden.append(layer)\n\n",
        "        self.output_layer = tf.keras.layers.Dense(\n",
        "            layers[-1],\n",
        "            kernel_initializer='glorot_normal',\n",
        "            bias_initializer='zeros',\n",
        "            activation=None\n",
        "        )\n\n",
        "    def call(self, inputs):\n",
        "        t, r = inputs[:, 0:1], inputs[:, 1:2]\n",
        "        t_norm = t / self.params.t_scale\n",
        "        r_norm = r / self.params.r_scale\n",
        "        x = tf.concat([t_norm, r_norm], axis=1)\n\n",
        "        for layer in self.hidden:\n",
        "            x = layer(x)\n",
        "        u_raw = self.output_layer(x)\n\n",
        "        # Apply sigmoid activation to ensure positive outputs\n",
        "        u_scaled = tf.nn.sigmoid(u_raw)\n",
        "        return u_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PDE Residuals\n",
        "The `get_pde_residuals` function computes the residuals of the PDEs using automatic differentiation, ensuring numerical stability with epsilon terms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_pde_residuals(model, t, r, params):\n",
        "    epsilon = 1e-8\n",
        "    t_tensor = tf.convert_to_tensor(t, dtype=tf.float32)\n",
        "    r_tensor = tf.convert_to_tensor(r, dtype=tf.float32)\n\n",
        "    # Ensure r is never exactly zero\n",
        "    r_tensor = tf.maximum(r_tensor, epsilon)\n\n",
        "    with tf.GradientTape(persistent=True) as tape1:\n",
        "        tape1.watch([t_tensor, r_tensor])\n",
        "        with tf.GradientTape(persistent=True) as tape2:\n",
        "            tape2.watch([t_tensor, r_tensor])\n\n",
        "            inputs = tf.concat([t_tensor, r_tensor], axis=1)\n",
        "            u_scaled = model(inputs)\n\n",
        "            # Scale back to physical units\n",
        "            Nn = u_scaled[:, 0:1] * params.Nn_scale\n",
        "            Nt = u_scaled[:, 1:2] * params.Nt_scale\n",
        "            Ch = u_scaled[:, 2:3] * params.Ch_scale\n\n",
        "        # First derivatives\n",
        "        Nn_t = tape2.gradient(Nn, t_tensor)\n",
        "        Nt_t = tape2.gradient(Nt, t_tensor)\n",
        "        Ch_t = tape2.gradient(Ch, t_tensor)\n\n",
        "        Nn_r = tape2.gradient(Nn, r_tensor)\n",
        "        Nt_r = tape2.gradient(Nt, r_tensor)\n",
        "        Ch_r = tape2.gradient(Ch, r_tensor)\n\n",
        "    # Second derivatives\n",
        "    Nt_rr = tape1.gradient(Nt_r, r_tensor)\n",
        "    Ch_rr = tape1.gradient(Ch_r, r_tensor)\n\n",
        "    # Clean up tapes\n",
        "    del tape1, tape2\n\n",
        "    # Check for None gradients and handle them\n",
        "    if Nn_t is None: Nn_t = tf.zeros_like(Nn)\n",
        "    if Nt_t is None: Nt_t = tf.zeros_like(Nt)\n",
        "    if Ch_t is None: Ch_t = tf.zeros_like(Ch)\n",
        "    if Nn_r is None: Nn_r = tf.zeros_like(Nn)\n",
        "    if Nt_r is None: Nt_r = tf.zeros_like(Nt)\n",
        "    if Ch_r is None: Ch_r = tf.zeros_like(Ch)\n",
        "    if Nt_rr is None: Nt_rr = tf.zeros_like(Nt)\n",
        "    if Ch_rr is None: Ch_rr = tf.zeros_like(Ch)\n\n",
        "    # Add small epsilon to prevent division by zero\n",
        "    Nn = tf.maximum(Nn, epsilon)\n",
        "    Nt = tf.maximum(Nt, epsilon)\n",
        "    Ch = tf.maximum(Ch, epsilon)\n\n",
        "    # PDE residuals with improved numerical stability\n",
        "    f_Nn = Nn_t - (params.rn1 * Nn * (1 - Nn / params.Kn) - params.rn2 * Ch * Nn)\n\n",
        "    # Diffusion coefficient with better bounds\n",
        "    D = tf.maximum(params.Dt * (1 - Nn / params.Kn), 0.0)\n\n",
        "    # Handle r=0 case more carefully\n",
        "    r_safe = tf.maximum(r_tensor, epsilon)\n\n",
        "    # Diffusion terms with safer computation\n",
        "    diffusion_Nt = D * (Nt_rr + 2.0 / r_safe * Nt_r)\n",
        "    diffusion_Ch = params.Dh * (Ch_rr + 2.0 / r_safe * Ch_r)\n\n",
        "    # Special case for r ≈ 0 (use L'Hôpital's rule result)\n",
        "    is_near_zero = tf.less(r_tensor, epsilon * 100)\n",
        "    diffusion_Nt_r0 = 3.0 * D * Nt_rr\n",
        "    diffusion_Ch_r0 = 3.0 * params.Dh * Ch_rr\n\n",
        "    diffusion_Nt = tf.where(is_near_zero, diffusion_Nt_r0, diffusion_Nt)\n",
        "    diffusion_Ch = tf.where(is_near_zero, diffusion_Ch_r0, diffusion_Ch)\n\n",
        "    f_Nt = Nt_t - (params.rt1 * Nt * (1 - Nt / params.Kt) + diffusion_Nt)\n",
        "    f_Ch = Ch_t - (params.rh1 * Nt - params.rh2 * Ch + diffusion_Ch)\n\n",
        "    return f_Nn, f_Nt, f_Ch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training the PINN\n",
        "The `run_pinn_training_tf` function orchestrates the training process, including data preparation, loss computation, and optimization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def safe_gradient(model, X, output_index):\n",
        "    \"\"\"Helper function to safely compute gradients with None checks.\"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(X)\n",
        "        u = model(X)\n",
        "        output = u[:, output_index:output_index+1]\n",
        "    grad = tape.gradient(output, X)\n",
        "    if grad is None:\n",
        "        return tf.zeros_like(X)\n",
        "    return grad\n\n",
        "def run_pinn_training_tf():\n",
        "    print(f\"--- Starting PINN Training with TensorFlow ---\")\n",
        "    params = ModelParameters()\n",
        "    t_data, r_data, Nn_clean, Nt_clean, Ch_clean, Nn_noisy, Nt_noisy, Ch_noisy = generate_ground_truth_data(params)\n\n",
        "    # Prepare training data\n",
        "    t_flat = np.tile(t_data, (params.n, 1)).T.flatten()[:, np.newaxis]\n",
        "    r_flat = np.tile(r_data, (len(t_data), 1)).flatten()[:, np.newaxis]\n\n",
        "    X_data = tf.constant(np.hstack([t_flat, r_flat]), dtype=tf.float32)\n\n",
        "    # Scale to [0,1] range for sigmoid output\n",
        "    U_data_scaled = tf.constant(np.hstack([\n",
        "        Nn_noisy.flatten()[:,np.newaxis] / params.Nn_scale,\n",
        "        Nt_noisy.flatten()[:,np.newaxis] / params.Nt_scale,\n",
        "        Ch_noisy.flatten()[:,np.newaxis] / params.Ch_scale\n",
        "    ]), dtype=tf.float32)\n\n",
        "    # Initial conditions\n",
        "    u0_full = get_initial_conditions(params)\n",
        "    X_ic = tf.constant(np.hstack([np.zeros_like(params.r)[:,np.newaxis], params.r[:,np.newaxis]]), dtype=tf.float32)\n",
        "    U_ic_scaled = tf.constant(np.hstack([\n",
        "        u0_full[0:params.n][:,np.newaxis] / params.Nn_scale,\n",
        "        u0_full[params.n:2*params.n][:,np.newaxis] / params.Nt_scale,\n",
        "        u0_full[2*params.n:3*params.n][:,np.newaxis] / params.Ch_scale\n",
        "    ]), dtype=tf.float32)\n\n",
        "    # Boundary conditions - left (r=0) and right (r=0.5)\n",
        "    t_left = tf.random.uniform((500, 1), 0, 5.0e+06, dtype=tf.float32)\n",
        "    r_left = tf.zeros_like(t_left) + params.r_min  # Small value near zero\n",
        "    X_left = tf.concat([t_left, r_left], axis=1)\n\n",
        "    t_right = tf.random.uniform((500, 1), 0, 5.0e+06, dtype=tf.float32)\n",
        "    r_right = tf.ones_like(t_right) * 0.5\n",
        "    X_right = tf.concat([t_right, r_right], axis=1)\n\n",
        "    # Collocation points\n",
        "    N_collocation = 2000\n",
        "    t_colloc = tf.random.uniform((N_collocation, 1), 0, 5.0e+06, dtype=tf.float32)\n",
        "    r_colloc = tf.random.uniform((N_collocation, 1), params.r_min, 0.5, dtype=tf.float32)\n\n",
        "    # Network architecture\n",
        "    layers = [2, 80, 80, 80, 3]\n",
        "    pinn_net = PINN(layers, params)\n\n",
        "    # Optimizer\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n\n",
        "    @tf.function\n",
        "    def train_step():\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Data loss\n",
        "            u_pred_scaled = pinn_net(X_data)\n",
        "            loss_data = tf.reduce_mean(tf.square(u_pred_scaled - U_data_scaled))\n\n",
        "            # Initial condition loss\n",
        "            u_ic_pred_scaled = pinn_net(X_ic)\n",
        "            loss_ic = tf.reduce_mean(tf.square(u_ic_pred_scaled - U_ic_scaled))\n\n",
        "            # Boundary condition losses\n",
        "            Nt_r_left = safe_gradient(pinn_net, X_left, 1)[:, 1:2]  # dNt/dr at r=0\n",
        "            Ch_r_left = safe_gradient(pinn_net, X_left, 2)[:, 1:2]  # dCh/dr at r=0\n\n",
        "            Nt_r_right = safe_gradient(pinn_net, X_right, 1)[:, 1:2]  # dNt/dr at r=0.5\n",
        "            Ch_r_right = safe_gradient(pinn_net, X_right, 2)[:, 1:2]  # dCh/dr at r=0.5\n\n",
        "            loss_bc = (tf.reduce_mean(tf.square(Nt_r_left)) +\n",
        "                       tf.reduce_mean(tf.square(Ch_r_left)) +\n",
        "                       tf.reduce_mean(tf.square(Nt_r_right)) +\n",
        "                       tf.reduce_mean(tf.square(Ch_r_right)))\n\n",
        "            # PDE loss\n",
        "            f_Nn, f_Nt, f_Ch = get_pde_residuals(pinn_net, t_colloc, r_colloc, params)\n",
        "            loss_pde = (tf.reduce_mean(tf.square(f_Nn)) +\n",
        "                        tf.reduce_mean(tf.square(f_Nt)) +\n",
        "                        tf.reduce_mean(tf.square(f_Ch)))\n\n",
        "            # Combined loss with weighting\n",
        "            total_loss = (500 * loss_data +\n",
        "                          50 * loss_ic +\n",
        "                          50 * loss_bc +\n",
        "                           1 * loss_pde)\n\n",
        "        gradients = tape.gradient(total_loss, pinn_net.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, pinn_net.trainable_variables))\n",
        "        return total_loss, loss_data, loss_ic, loss_bc, loss_pde\n\n",
        "    # Training loop\n",
        "    epochs = 10000\n\n",
        "    for epoch in range(epochs):\n",
        "        total_loss, loss_data, loss_ic, loss_bc, loss_pde = train_step()\n\n",
        "        if epoch % 200 == 0:\n",
        "            print(f\"Epoch {epoch}: Total Loss: {total_loss.numpy():.4e}, \"\n",
        "                  f\"Data: {loss_data.numpy():.4e}, IC: {loss_ic.numpy():.4e}, \"\n",
        "                  f\"BC: {loss_bc.numpy():.4e}, PDE: {loss_pde.numpy():.4e}\")\n\n",
        "    print(\"\\n--- Evaluating Trained PINN Model ---\")\n",
        "    u_pinn_scaled = pinn_net(X_data).numpy()\n",
        "    u_pinn_pred = np.hstack([\n",
        "        u_pinn_scaled[:, 0:1] * params.Nn_scale,\n",
        "        u_pinn_scaled[:, 1:2] * params.Nt_scale,\n",
        "        u_pinn_scaled[:, 2:3] * params.Ch_scale,\n",
        "    ]).reshape(len(t_data), params.n, 3)\n\n",
        "    Nn_pinn, Nt_pinn, Ch_pinn = u_pinn_pred[:,:,0], u_pinn_pred[:,:,1], u_pinn_pred[:,:,2]\n\n",
        "    # Calculate errors\n",
        "    def safe_l2_error(pred, true):\n",
        "        diff_norm = np.linalg.norm(true - pred)\n",
        "        true_norm = np.linalg.norm(true)\n",
        "        return diff_norm / max(true_norm, 1e-10)\n\n",
        "    l2_error_Nn = safe_l2_error(Nn_pinn, Nn_clean)\n",
        "    l2_error_Nt = safe_l2_error(Nt_pinn, Nt_clean)\n",
        "    l2_error_Ch = safe_l2_error(Ch_pinn, Ch_clean)\n\n",
        "    print(f\"Relative L2 Error (Nn): {l2_error_Nn:.4e}\")\n",
        "    print(f\"Relative L2 Error (Nt): {l2_error_Nt:.4e}\")\n",
        "    print(f\"Relative L2 Error (Ch): {l2_error_Ch:.4e}\")\n\n",
        "    # Calculate and Print Overall Accuracy\n",
        "    mean_relative_error = (l2_error_Nn + l2_error_Nt + l2_error_Ch) / 3.0\n",
        "    accuracy = (1.0 - mean_relative_error) * 100\n\n",
        "    print(\"\\n-------------------------------------\")\n",
        "    print(f\"📊 Overall Model Accuracy: {accuracy:.2f}%\")\n",
        "    print(\"-------------------------------------\")\n\n",
        "    # Plotting\n",
        "    time_indices_to_plot = [0, 5, 10, 15, 20]\n",
        "    fig, axs = plt.subplots(3, len(time_indices_to_plot), figsize=(20, 12), sharey='row')\n",
        "    fig.suptitle('PINN vs. Numerical Solution Comparison (Training on Noisy Data)', fontsize=16)\n\n",
        "    for i, t_idx in enumerate(time_indices_to_plot):\n",
        "        t_days = t_data[t_idx] / (60 * 60 * 24)\n\n",
        "        axs[0, i].set_title(f't = {t_days:.1f} days')\n",
        "        axs[0, i].plot(r_data, Nn_clean[t_idx, :], 'b-', label='Numerical (Clean)')\n",
        "        axs[0, i].plot(r_data, Nn_noisy[t_idx, :], 'k.', markersize=2, label='Training Data (Noisy)')\n",
        "        axs[0, i].plot(r_data, Nn_pinn[t_idx, :], 'r--', label='PINN Prediction')\n",
        "        if i == 0: axs[0, i].set_ylabel('Nn (normal cells)')\n\n",
        "        axs[1, i].plot(r_data, Nt_clean[t_idx, :], 'b-')\n",
        "        axs[1, i].plot(r_data, Nt_noisy[t_idx, :], 'k.', markersize=2)\n",
        "        axs[1, i].plot(r_data, Nt_pinn[t_idx, :], 'r--')\n",
        "        if i == 0: axs[1, i].set_ylabel('Nt (tumor cells)')\n\n",
        "        axs[2, i].plot(r_data, Ch_clean[t_idx, :], 'b-')\n",
        "        axs[2, i].plot(r_data, Ch_noisy[t_idx, :], 'k.', markersize=2)\n",
        "        axs[2, i].plot(r_data, Ch_pinn[t_idx, :], 'r--')\n",
        "        if i == 0: axs[2, i].set_ylabel('Ch (excess H+)')\n",
        "        axs[2, i].set_xlabel('r (cm)')\n\n",
        "    handles, labels = axs[0,0].get_legend_handles_labels()\n",
        "    fig.legend(handles, labels, loc='upper right')\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n\n",
        "    # Save and show plot\n",
        "    plot_filename = 'pinn_comparison.png'\n",
        "    plt.savefig(plot_filename)\n",
        "    plt.show()\n",
        "    print(f\"Plot saved as {plot_filename}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execution\n",
        "Run the PINN training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if __name__ == '__main__':\n",
        "    run_pinn_training_tf()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Outputs\n",
        "- **CSV File**: `tumor_growth_data.csv` containing noisy ground truth data.\n",
        "- **Plot**: `pinn_comparison.png` comparing PINN predictions with numerical solutions.\n",
        "- **Console Output**: Training progress, relative L2 errors, and overall model accuracy."
      ]
    }
  ]
}